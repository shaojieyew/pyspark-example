{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59698ee8-e6a9-4d18-b250-fa6349ddb66c",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "- Why Distributed File System and Computing?\n",
    "\t- Distributed File System\n",
    "\t- Distributed Computing\n",
    "\t- Resource Manager \n",
    "\n",
    "- What is Spark?\n",
    "\t- RDD\n",
    "\t- Dataframe & SQL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8616a7-0cae-4a59-83ca-d4c32479cecb",
   "metadata": {},
   "source": [
    "## Why Distributed File System and Computing?\n",
    "\n",
    "- Solve Big Data Problem; \n",
    "  where data too big to store or process data in a single machine\n",
    "- Increase Parallelism in Computing\n",
    "- Higher Availability\n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\1_single_node_filesystem.png\" width=\"400\"><img src=\"doc\\1_spark_introduction\\2_distributed_filesystem.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef888e-d516-4d98-928f-3d808c730084",
   "metadata": {},
   "source": [
    "### Distributed File System\n",
    "##### Example DFS\n",
    "- Hadoop Distributed File System\n",
    "- MinIO\n",
    "- Isilon OneFS\n",
    "- Amazon S3\n",
    "- Google GCS\n",
    "|\n",
    "In DFS, a file larger than a predefined block size is split into blocks and then to disks \n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\3_hdfs_filesystem.png\" width=\"400\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2bf69-57a2-471f-84e9-e2bb8b3ad919",
   "metadata": {},
   "source": [
    "### Distributed Computing\n",
    "\n",
    "- Uses functional programming approach (stateless design easily)\n",
    "\t- Map, Reduce, Filter\n",
    "- Enable Parallelism\n",
    "\t- Utilizing multiple processors\n",
    "- Example Distributed Processing Framework\n",
    "\t- MapReduce\n",
    "\t- Spark\n",
    "\t- Flink\n",
    "\t- Beam\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2988da-53a5-459e-8b68-fdf78ee60527",
   "metadata": {},
   "source": [
    "\n",
    "#### Word Count Example\n",
    "<img src=\"doc\\1_spark_introduction\\4_mapreduce_wordcount.png\" width=\"800\">\n",
    "\n",
    "\n",
    "#### Left Outer Join Example\n",
    "<img src=\"doc\\1_spark_introduction\\5_mapreduce_join.png\" width=\"700\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc185b6f-46d9-48a9-8d80-5241aaf07585",
   "metadata": {},
   "source": [
    "### Resource Manager\n",
    "\n",
    "It handles the resource management and scheduling of containers during computing\n",
    "\n",
    "##### Example Resource Manager Supported by Spark:\n",
    "- Apache Hadoop YARN\n",
    "- Kubernetes\n",
    "- Mesos\n",
    "- Docker Swarm\n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\6_yarn_cluster_workflow.png\" width=\"500\"><img src=\"doc\\1_spark_introduction\\7_kube_workflow_spark.png\" width=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb2030-8f4e-468d-810f-6537881ca948",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "Apache Spark isÂ  a data processing framework\n",
    "- Unified analytics engine \n",
    "- Process tasks on large-scale datasets across multiple computers\n",
    "- Fault tolerance\n",
    "- Provides easy-to-use interface for distributed computing with implicit data parallelism \n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\8_spark_stack.png\" width=\"500\">\n",
    "\n",
    "Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop MapReduce\n",
    "\n",
    "| Spark   | Hadoop MapReduce    |\n",
    "|-------------|-------------|\n",
    "| <img src=\"doc\\1_spark_introduction\\9_spark_highlevel_workflow.jpg\" width=\"600\">         |  <img src=\"doc\\1_spark_introduction\\10_hadoop_mapreduce_workflow.png\" width=\"460\">         |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0cb20-c2a6-49ab-87a6-978f45f4db2a",
   "metadata": {},
   "source": [
    "## Spark Basics - RDD\n",
    "\n",
    "RDD (Resilient Distributed Dataset) Is data structure of Apache Spark. \n",
    "- It is an immutable collection of objects which computes on the different node of the cluster.\n",
    "- Resilient, restore the data on failure by using RDD lineage graph (DAG), \n",
    "\t-  recompute missing or damaged partitions due to node failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f66db-5bea-4e1d-8cdc-adc717a562ff",
   "metadata": {},
   "source": [
    "### Start SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "3340e609-4ce3-4e0b-ac5c-e83b92c303d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://DESKTOP-1A80NHD.mshome.net:4040'"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"simpleApp\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.sc().uiWebUrl().get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e2c2a0-5ced-48c4-9c22-1d07c23f246c",
   "metadata": {},
   "source": [
    "### Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "85559400-0096-4daa-9930-0e9402575c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['First sample data', 'Also a sample']\n"
     ]
    }
   ],
   "source": [
    "data = [\"First sample data\", \"Also a sample\"]\n",
    "rdd = sc.parallelize(data)\n",
    "print(type(rdd))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "4281adb2-91eb-498f-9d27-f4c8d1bdd500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"./data/raw\")\n",
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "8ae81ae4-1bcd-44ed-9449-11ccc9c882b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "collected_list = rdd.take(5)\n",
    "print(type(collected_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "526160c9-4277-4069-a847-2565fc4fab2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The park have statues that was crafted in the early 70s and have  foretold stories from Chinese mythology, folklore, legends etc. I have been here since my early childhood with my parent , then with my schoolmate  and later with family and children. Very nice place to walk asit fill with heritages.',\n",
       " 'Visited after reopening in 1 July. Although the two main sections (Hell museum, 10 stages) were not open it still had a couple of statues with descriptions. Seems like management is trying to open new f&b venues inside the park but not open at this point. Quite an exotic experience free of charge!',\n",
       " 'Good Place to know about the Chinese Ancient Scripts, Tradition and their Sulptures. Most people feel this place is creepy but its not. The Architectures and the Sculptures are really good to admire. There is also a small Buddha temple inside. There is no proper restaurants though. Restrooms are available. Its free of charge, People visiting Chinese Garden also should visit this place.',\n",
       " 'Many scared of you know tiger balm brand. Well this villa is built scared by the founder. Inside, you can find so many Chinese teachings and legends, including also Buddhism aspects.',\n",
       " 'The particular one is the 10 courts of hell where they describe in the scary way every court of Chinese or Buddhist hell. You can even see the type of punishment illustrated in detailed statues.']"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "ea688fa0-345f-41e6-8d1c-327f9728140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in rdd = 39\n",
      "number of partitions = 3\n",
      "number of records in each partition = [(0, 21), (1, 14), (2, 4)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def describeRdd(rdd, description = \"\"):\n",
    "    def countRecord(splitIndex, iterator): \n",
    "        yield (splitIndex, len(list(iterator)))\n",
    "    numRecordPerPartition = rdd.mapPartitionsWithIndex(countRecord).collect()\n",
    "    if (len(description)>0):\n",
    "        print(description)\n",
    "    print(f\"number of records in rdd = {rdd.count()}\")\n",
    "    print(f\"number of partitions = { rdd.getNumPartitions()}\")\n",
    "    print(f\"number of records in each partition = {numRecordPerPartition}\")\n",
    "    print(\"\")\n",
    "    \n",
    "describeRdd(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588cbf35-f40a-4974-bc42-04fca9d957c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### RDD Transformations & Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa168b-0e0b-48fc-8ff8-59766f20574e",
   "metadata": {},
   "source": [
    "Spark TransformationÂ is a function that produces new RDD from the existing RDDs.Â \n",
    "- Lazy evaluation\n",
    "\t- Data does not get loaded in an RDD during transformation\n",
    "\t- RDD keep a reference that points to previous RDD; aka RDD lineage \n",
    "\n",
    "Spark Action\n",
    "- Transformations are only computed when an action is called\n",
    "- Spark can make optimization decision after it looks at the RDD lineage; avoid large volume of Network IO\n",
    "- The Materialized RDD result is stored in memory for next action to be triggered\n",
    "\n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\11_spark_rdd_transfromation_action.png\" width=\"600\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939fa03-d3ec-4f74-85cf-56bfed97e4a9",
   "metadata": {},
   "source": [
    "#### Example of RDD Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "f1772532-b23c-4d8f-ab7a-ec8465863d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_words(words):\n",
    "    return map(lambda word: re.sub(\"[^0-9a-zA-Z ]+\", \"\", word), words)\n",
    "\n",
    "def is_insignificant(word):\n",
    "    if(word in [\"the\",\"and\",\"to\",\"of\",\"a\",\"is\",\"it\",\"in\",\"this\",\"my\",\"you\",\"as\",\"its\",\"are\",\"was\",\"were\",\"can\",\"i\",\"but\",\"with\",\"and\",\"an\",\"there\",\"here\",\"has\",\"not\",\"many\",\"for\"]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "tokenized_rdd = rdd.map(lambda record: re.split(\"\\s|(?<!\\d)[,.](?!\\d)\", record))\n",
    "cleaned_rdd = tokenized_rdd.flatMap(clean_words)\n",
    "cleaned_non_empty_rdd = cleaned_rdd.filter(lambda word: len(word)>0)\n",
    "normalized_rdd = cleaned_non_empty_rdd.map(lambda word: word.lower())\n",
    "cleaned_rdd = normalized_rdd.filter(lambda word: not(is_insignificant(word)))\n",
    "\n",
    "cleaned_rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f4337-7235-4997-b414-026ccc49182e",
   "metadata": {},
   "source": [
    "#### Example of RDD Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "692d6e24-679e-4acb-81ef-951f36b9590b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count is an action\n",
    "cleaned_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "38e3865f-eb38-4597-a073-57d0e9be7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_rdd = cleaned_rdd.map(lambda x : (x,1)).aggregateByKey(0, lambda x, y: x+y , lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "1183078f-50b7-4830-8437-35c7f04d974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('park', 22),\n",
       " ('place', 22),\n",
       " ('chinese', 19),\n",
       " ('free', 14),\n",
       " ('statues', 13),\n",
       " ('hell', 13),\n",
       " ('scared', 12),\n",
       " ('visit', 12),\n",
       " ('scary', 10),\n",
       " ('villa', 9),\n",
       " ('par', 9),\n",
       " ('tiger', 9),\n",
       " ('very', 8),\n",
       " ('balm', 8),\n",
       " ('interesting', 8)]"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect is an action\n",
    "word_count_list = word_count_rdd.collect()\n",
    "word_count_list.sort(key= lambda kv: kv[1], reverse=True)\n",
    "word_count_list[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e177c-8274-4e9a-a3d5-f0f91c93c374",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### RDD - DAG and Lineage\n",
    "Directed Acyclic Graph is a finite direct graph that performs a sequence of computations on data. \n",
    "Each node is an RDD, and the edge is a transformation on top of data.\n",
    "\n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\12_rdd_dag_lineage_example.png\" width=\"400\">   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef763ff6-bfbb-4271-a3d2-a10bfec61504",
   "metadata": {},
   "source": [
    "\n",
    "### RDD Cache/Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "6bb36be8-06d5-470e-8d85-e43bdba05b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rdd.persist()\n",
    "word_count_rdd = cleaned_rdd.map(lambda x : (x,1)).aggregateByKey(0, lambda x, y: x+y , lambda x,y : x+y)\n",
    "word_count_list = word_count_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88bbe1-8b5d-4075-a47e-e4dc9d6812f9",
   "metadata": {},
   "source": [
    "### RDD Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "7be8ae29-d3aa-4251-9186-5f9db647514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_rdd\n",
      "number of records in rdd = 39\n",
      "number of partitions = 3\n",
      "number of records in each partition = [(0, 21), (1, 14), (2, 4)]\n",
      "\n",
      "cleaned_rdd\n",
      "number of records in rdd = 1068\n",
      "number of partitions = 3\n",
      "number of records in each partition = [(0, 566), (1, 364), (2, 138)]\n",
      "\n",
      "cleaned_non_empty_rdd\n",
      "number of records in rdd = 1618\n",
      "number of partitions = 3\n",
      "number of records in each partition = [(0, 866), (1, 540), (2, 212)]\n",
      "\n",
      "normalized_rdd\n",
      "number of records in rdd = 1618\n",
      "number of partitions = 3\n",
      "number of records in each partition = [(0, 866), (1, 540), (2, 212)]\n",
      "\n",
      "word_count_rdd\n",
      "number of records in rdd = 531\n",
      "number of partitions = 3\n",
      "number of records in each partition = [(0, 196), (1, 161), (2, 174)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describeRdd(tokenized_rdd,\"tokenized_rdd\")\n",
    "describeRdd(cleaned_rdd,\"cleaned_rdd\")\n",
    "describeRdd(cleaned_non_empty_rdd,\"cleaned_non_empty_rdd\")\n",
    "describeRdd(normalized_rdd,\"normalized_rdd\")\n",
    "describeRdd(word_count_rdd,\"word_count_rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "30b807d1-7d83-4298-ad98-a0489b04b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_non_empty_rdd\n",
      "number of records in rdd = 1618\n",
      "number of partitions = 6\n",
      "number of records in each partition = [(0, 270), (1, 272), (2, 270), (3, 266), (4, 270), (5, 270)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_non_empty_rdd = cleaned_non_empty_rdd.repartition(6)\n",
    "\n",
    "describeRdd(cleaned_non_empty_rdd,\"cleaned_non_empty_rdd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ab36a-0160-4280-a40c-bc98f9f20ba3",
   "metadata": {},
   "source": [
    "### Write RDD to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "6c339950-b365-4b8b-b80b-9614aff382e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_rdd.map(lambda x: f\"{x[0]}, {x[1]}, {x[0][:1]}\") \\\n",
    "  .saveAsTextFile(\"data/rdd_example/word_count_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51cc70-ed87-4ce5-879b-4bb0ee13d92c",
   "metadata": {},
   "source": [
    "---\n",
    "## SparkSQL and Dataframe\n",
    "\n",
    "Spark SQL is a module for working with structured data\n",
    "- Unified Data Access\n",
    "\t- Data Sources like Hive, Avro, Parquet, ORC, Kafka , JSON, as well as JDBC\n",
    "- Enable SQL queries\n",
    "- Query Plan Optimization\n",
    "- Abstraction of batch and streaming operations\n",
    "\n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\13_spark_architecture.png\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f2dba-6e21-46cd-ac07-c30f8ed1743c",
   "metadata": {},
   "source": [
    "### Dataframe\n",
    "DataFrames is an integrated data structure with an easy-to-use API for simplifying distributed big data processing. \n",
    "- Can be constructed from many sources including structured data files, tables in Hive, external databases, or existing RDDs\n",
    "- Provide SQL like data manipulations and aggregations\n",
    "- It is an extension of the Spark RDD API optimized for writing code more efficiently\n",
    "- Similar to Python PandasÂ and R data frames. Except is made to integrate with large-scale data and optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2e4c2-4a6b-4f70-a0ac-bff427d8eef6",
   "metadata": {},
   "source": [
    "### Create Dataframe from Python List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "ead0c475-fee1-45f1-bed6-76c47de4b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "data_1 = [\n",
    "            {\"Category\": 'A', \"ID\": 1, \"Value\": 121.44, \"Truth\": True},\n",
    "            {\"Category\": 'B', \"ID\": 2, \"Value\": 300.01, \"Truth\": False},\n",
    "            {\"Category\": 'C', \"ID\": 3, \"Value\": 10.99, \"Truth\": None},\n",
    "            {\"Category\": 'E', \"ID\": 4, \"Value\": 33.87, \"Truth\": True}\n",
    "        ]\n",
    "\n",
    "schema_1 = StructType([\n",
    "    StructField(dataType=StringType(), name=\"Category\"),\n",
    "    StructField(dataType=IntegerType(), name=\"ID\"),\n",
    "    StructField(dataType=DoubleType(), name=\"Value\"),\n",
    "    StructField(dataType=BooleanType(), name=\"Truth\"),\n",
    "])\n",
    "df1 = spark.createDataFrame(data_1, schema=schema_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "81d5624e-0fb1-4159-aacc-f30e1a56917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+-----+\n",
      "|Category| ID| Value|Truth|\n",
      "+--------+---+------+-----+\n",
      "|       A|  1|121.44| true|\n",
      "|       B|  2|300.01|false|\n",
      "|       C|  3| 10.99| null|\n",
      "|       E|  4| 33.87| true|\n",
      "+--------+---+------+-----+\n",
      "\n",
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Value: double (nullable = true)\n",
      " |-- Truth: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac33106-31f0-40df-9697-2147c5994ea2",
   "metadata": {},
   "source": [
    "### Create Dataframe from a source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "715f1f15-1994-4f7b-a9a0-4ed93dd41a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(dataType=StringType(), name=\"word\"),\n",
    "    StructField(dataType=IntegerType(), name=\"count\"),\n",
    "    StructField(dataType=StringType(), name=\"starting_letter\"),\n",
    "])\n",
    "word_count_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"header\",False)\\\n",
    "    .schema(schema)\\\n",
    "    .load(\"data/rdd_example/word_count_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "35a7783e-2544-4bbb-bfe7-b0a62f79735f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------------+\n",
      "|      word|count|starting_letter|\n",
      "+----------+-----+---------------+\n",
      "|      have|    5|              h|\n",
      "|   statues|   13|              s|\n",
      "|      from|    5|              f|\n",
      "|   legends|    5|              l|\n",
      "|       etc|    1|              e|\n",
      "|    family|    2|              f|\n",
      "|  children|    3|              c|\n",
      "|      very|    8|              v|\n",
      "|      nice|    7|              n|\n",
      "|      walk|    2|              w|\n",
      "|   visited|    3|              v|\n",
      "| reopening|    1|              r|\n",
      "|  although|    1|              a|\n",
      "|       two|    1|              t|\n",
      "|      hell|   13|              h|\n",
      "|    museum|    2|              m|\n",
      "|      open|    3|              o|\n",
      "|     still|    4|              s|\n",
      "|      like|    6|              l|\n",
      "|management|    1|              m|\n",
      "+----------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb12f6b-f8e5-42e9-8a94-59b5e9964afb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Transform Data using SparkSQL & Dataframe API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ff658-70c2-44ce-97a5-0c4a173b4382",
   "metadata": {},
   "source": [
    "Create a tempview from dataframe so that it can be queried via SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "6d266285-7c71-4ef6-960d-692481c22f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='testing_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='word_count', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.createOrReplaceTempView(\"testing_table\")\n",
    "word_count_df.createOrReplaceTempView(\"word_count\")\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0460ddb-5f20-430a-aa6c-455ac6746415",
   "metadata": {},
   "source": [
    "#### SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "13b8bf59-8748-4e36-857a-a12b9463e664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|VILLA|    9|\n",
      "|TIGER|    9|\n",
      "| BALM|    8|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_sql_query = spark.sql(\"\"\"\n",
    "    SELECT UPPER(word) as word, count\n",
    "    FROM word_count \n",
    "    WHERE (word IN ('tiger','balm', 'villa'))\n",
    "        AND (count > 5)\n",
    "    ORDER BY word DESC\n",
    "\"\"\")\n",
    "dataframe_sql_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80d32f-e0ee-475a-bf9c-b1c7ec8826d8",
   "metadata": {},
   "source": [
    "#### Pyspark Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "24e003a2-d744-433d-8a61-e601c5320f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|VILLA|    9|\n",
      "|TIGER|    9|\n",
      "| BALM|    8|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = word_count_df.filter((col(\"word\").isin([\"tiger\",\"villa\",\"balm\"])))\n",
    "df = df.withColumn(\"word\", upper(col(\"word\")))\n",
    "df = df.orderBy(\"word\", ascending=False)\n",
    "df = df.filter(col(\"count\") > 5)\n",
    "df = df.select(col(\"word\"), col(\"count\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13a778-93f0-4782-ad6a-1b5f2dca9178",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Catalyst Optimizer\n",
    "\n",
    "<img src=\"doc\\1_spark_introduction\\14_catalyst_optimizer.png\" width=\"800\"> \n",
    "\n",
    "- Catalyst Optimizer engine converts each SQL query into a logical plan. \n",
    "- and then converts it to many physical execution plans. Â \n",
    "    - eg. Projection and Predicate Pushdown\n",
    "- During execution, it selects the most optimal physical plan and generates new RDD lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "638ee7a0-afc3-4b9c-8233-e09b3582794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('word, None), unresolvedalias('count, None)]\n",
      "+- Filter (count#1808 > 5)\n",
      "   +- Sort [word#2173 DESC NULLS LAST], true\n",
      "      +- Project [upper(word#1807) AS word#2173, count#1808, starting_letter#1809]\n",
      "         +- Filter word#1807 IN (tiger,villa,balm)\n",
      "            +- Relation[word#1807,count#1808,starting_letter#1809] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "word: string, count: int\n",
      "Project [word#2173, count#1808]\n",
      "+- Filter (count#1808 > 5)\n",
      "   +- Sort [word#2173 DESC NULLS LAST], true\n",
      "      +- Project [upper(word#1807) AS word#2173, count#1808, starting_letter#1809]\n",
      "         +- Filter word#1807 IN (tiger,villa,balm)\n",
      "            +- Relation[word#1807,count#1808,starting_letter#1809] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [word#2173 DESC NULLS LAST], true\n",
      "+- Project [upper(word#1807) AS word#2173, count#1808]\n",
      "   +- Filter ((isnotnull(count#1808) && word#1807 IN (tiger,villa,balm)) && (count#1808 > 5))\n",
      "      +- Relation[word#1807,count#1808,starting_letter#1809] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Sort [word#2173 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(word#2173 DESC NULLS LAST, 200)\n",
      "   +- *(1) Project [upper(word#1807) AS word#2173, count#1808]\n",
      "      +- *(1) Filter ((isnotnull(count#1808) && word#1807 IN (tiger,villa,balm)) && (count#1808 > 5))\n",
      "         +- *(1) FileScan csv [word#1807,count#1808] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Workspace/Projects/spark/data/rdd_example/word_count_result], PartitionFilters: [], PushedFilters: [IsNotNull(count), In(word, [tiger,villa,balm]), GreaterThan(count,5)], ReadSchema: struct<word:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27498ecd-6978-44b4-bb93-ac6d46b1fb04",
   "metadata": {},
   "source": [
    "         \n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Original Plan</th>\n",
    "        <th>Optimized Plan</th>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>1. filter by \"tiger\",\"villa\",\"balm\" </td>\n",
    "        <td>1. filter by \"tiger\",\"villa\",\"balm\"  </td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>2. upper case \"word\" column </td>\n",
    "        <td>2. filter by \"count\" > 5 ðŸ”¼</td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>3. order by \"word\" column </td>\n",
    "        <td>3. select \"word\" and \"count\" column ðŸ”¼</td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>4. filter by \"count\" > 5 ðŸ”¼</td>\n",
    "        <td>4. upper case \"word\" column</td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>5. select \"word\" and \"count\" column ðŸ”¼</td>\n",
    "        <td>5. order by \"word\" column </td>\n",
    "   </tr>\n",
    "    </table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e5ba9-e388-4131-b3b3-a5d6074efd13",
   "metadata": {},
   "source": [
    "### Write Dataframe to File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "70843eb8-42d1-4dff-8a0a-f24341f6e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in rdd = 3\n",
      "number of partitions = 3\n",
      "number of records in each partition = [(0, 1), (1, 1), (2, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describeRdd(df.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "9ade0271-bf68-49ac-9aa1-6ba9a0204842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1) \\\n",
    "    .write.format(\"csv\") \\\n",
    "    .option(\"header\",True).save(\"data/dataframe_example/word_count_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb50b0-ddf2-43b8-8c5b-256cd8606461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea242a1-3709-41c8-93b9-0c5485f3502a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835c97c-3b4c-459a-93a9-ce90fc7c65ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
